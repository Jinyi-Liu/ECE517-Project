

@article{oroojlooyjadid_deep_2022,
	title = {A {Deep} {Q}-{Network} for the {Beer} {Game}: {Deep} {Reinforcement} {Learning} for {Inventory} {Optimization}},
	volume = {24},
	issn = {1523-4614},
	shorttitle = {A {Deep} {Q}-{Network} for the {Beer} {Game}},
	url = {https://pubsonline.informs.org/doi/10.1287/msom.2020.0939},
	doi = {10.1287/msom.2020.0939},
	abstract = {Problem definition: The beer game is widely used in supply chain management classes to demonstrate the bullwhip effect and the importance of supply chain coordination. The game is a decentralized, multiagent, cooperative problem that can be modeled as a serial supply chain network in which agents choose order quantities while cooperatively attempting to minimize the network’s total cost, although each agent only observes local information. Academic/practical relevance: Under some conditions, a base-stock replenishment policy is optimal. However, in a decentralized supply chain in which some agents act irrationally, there is no known optimal policy for an agent wishing to act optimally. Methodology: We propose a deep reinforcement learning (RL) algorithm to play the beer game. Our algorithm makes no assumptions about costs or other settings. As with any deep RL algorithm, training is computationally intensive, but once trained, the algorithm executes in real time. We propose a transfer-learning approach so that training performed for one agent can be adapted quickly for other agents and settings. Results: When playing with teammates who follow a base-stock policy, our algorithm obtains near-optimal order quantities. More important, it performs significantly better than a base-stock policy when other agents use a more realistic model of human ordering behavior. We observe similar results using a real-world data set. Sensitivity analysis shows that a trained model is robust to changes in the cost coefficients. Finally, applying transfer learning reduces the training time by one order of magnitude. Managerial implications: This paper shows how artificial intelligence can be applied to inventory optimization. Our approach can be extended to other supply chain optimization problems, especially those in which supply chain partners act in irrational or unpredictable ways. Our RL agent has been integrated into a new online beer game, which has been played more than 17,000 times by more than 4,000 people.},
	number = {1},
	urldate = {2023-11-01},
	journal = {Manufacturing \& Service Operations Management},
	author = {Oroojlooyjadid, Afshin and Nazari, MohammadReza and Snyder, Lawrence V. and Takáč, Martin},
	month = jan,
	year = {2022},
	note = {Publisher: INFORMS},
	keywords = {reinforcement learning, beer game, inventory optimization},
	pages = {285--304},
	file = {Full Text PDF:C\:\\Users\\jinyi\\Zotero\\storage\\MVJIXERU\\Oroojlooyjadid et al. - 2022 - A Deep Q-Network for the Beer Game Deep Reinforce.pdf:application/pdf},
}


@article{gijsbrechts_can_2022,
	title = {Can {Deep} {Reinforcement} {Learning} {Improve} {Inventory} {Management}? {Performance} on {Lost} {Sales}, {Dual}-{Sourcing}, and {Multi}-{Echelon} {Problems}},
	volume = {24},
	issn = {1523-4614},
	shorttitle = {Can {Deep} {Reinforcement} {Learning} {Improve} {Inventory} {Management}?},
	url = {https://pubsonline.informs.org/doi/10.1287/msom.2021.1064},
	doi = {10.1287/msom.2021.1064},
	abstract = {Problem definition: Is deep reinforcement learning (DRL) effective at solving inventory problems? Academic/practical relevance: Given that DRL has successfully been applied in computer games and robotics, supply chain researchers and companies are interested in its potential in inventory management. We provide a rigorous performance evaluation of DRL in three classic and intractable inventory problems: lost sales, dual sourcing, and multi-echelon inventory management. Methodology: We model each inventory problem as a Markov decision process and apply and tune the Asynchronous Advantage Actor-Critic (A3C) DRL algorithm for a variety of parameter settings. Results: We demonstrate that the A3C algorithm can match the performance of the state-of-the-art heuristics and other approximate dynamic programming methods. Although the initial tuning was computationally demanding and time demanding, only small changes to the tuning parameters were needed for the other studied problems. Managerial implications: Our study provides evidence that DRL can effectively solve stationary inventory problems. This is especially promising when problem-dependent heuristics are lacking. Yet, generating structural policy insight or designing specialized policies that are (ideally provably) near optimal remains desirable.},
	number = {3},
	urldate = {2023-11-01},
	journal = {Manufacturing \& Service Operations Management},
	author = {Gijsbrechts, Joren and Boute, Robert N. and Van Mieghem, Jan A. and Zhang, Dennis J.},
	month = may,
	year = {2022},
	note = {Publisher: INFORMS},
	keywords = {supply chain management, inventory theory and control, logistics and transportation, OM-information technology interface},
	pages = {1349--1368},
	file = {Full Text PDF:C\:\\Users\\jinyi\\Zotero\\storage\\2G5PI2MV\\Gijsbrechts et al. - 2022 - Can Deep Reinforcement Learning Improve Inventory .pdf:application/pdf},
}

@article{silver_mastering_2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	copyright = {2016 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	language = {en},
	number = {7587},
	urldate = {2023-12-11},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	note = {Number: 7587
Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science, Reward},
	pages = {484--489},
}
